{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from grid import location_to_cell_id\n",
    "from math import ceil\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Read Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pois_df = pd.read_csv('pois.csv', sep=' ')\n",
    "queries_df = pd.read_csv('queries.csv', sep=' ')\n",
    "\n",
    "print(pois_df)\n",
    "print(queries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Extract cell_id from which query happened\n",
    "def get_cell_id(row):\n",
    "    return location_to_cell_id(row['lat'], row['lon'])\n",
    "\n",
    "queries_df['cell_id'] = queries_df.apply(lambda row: get_cell_id(row), axis=1)\n",
    "print(queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Get day of the query and time\n",
    "def get_day(row):\n",
    "    return ceil(row['timestamp'] / 24)\n",
    "\n",
    "def get_hour_of_day(row):\n",
    "    return int(row['timestamp'] % 24)\n",
    "\n",
    "queries_df['day'] = queries_df.apply(lambda row: get_day(row), axis=1)\n",
    "queries_df['time'] = queries_df.apply(lambda row: get_hour_of_day(row), axis=1)\n",
    "\n",
    "#Get daytime\n",
    "def get_daytime(row):\n",
    "    time = row['time']\n",
    "    if (time >= 0 and time < 9):\n",
    "        return '1.Early'\n",
    "    if (time >= 9 and time < 12):\n",
    "        return '2.Morning'\n",
    "    if (time >= 12 and time < 17):\n",
    "        return '3.Afternoon'\n",
    "    if (time >= 17 and time < 20):\n",
    "        return '4.Evening'\n",
    "    if (time >= 20 and time < 24):\n",
    "        return '5.Night'\n",
    "queries_df['daytime'] = queries_df.apply(lambda row: get_daytime(row), axis=1)\n",
    "queries_df = queries_df.sort_values(by=['ip_address', 'day', 'time'])\n",
    "print(queries_df)\n",
    "queries_df.to_csv('queries_extended.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!rm queries_grouped.csv\n",
    "queries_grouped = queries_df.groupby(['ip_address', 'daytime', 'day','cell_id'])\\\n",
    "    .size().sort_index(level=[0,2,1])\n",
    "#queries_grouped = queries_grouped.where(queries_grouped['count'] > 4).dropna()\n",
    "\n",
    "queries_grouped = queries_grouped.to_frame(name='count').reset_index()\n",
    "print(queries_grouped)\n",
    "queries_grouped.to_csv('queries_grouped.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "queries_grouped contains, for each ip, for each day and daytime,\n",
    "the number of queries launched from a certain cell_id.\n",
    "It could be useful for inferring movement patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!rm max_cell_daytime.csv\n",
    "queries_grouped_day_removed = queries_grouped.drop(['day'],axis = 1)\n",
    "max_cell_daytime = queries_grouped_day_removed.groupby(['ip_address','daytime'])\\\n",
    "    .agg(['max'])\n",
    "max_cell_daytime.to_csv('max_cell_daytime.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_cell_daytime contains, for each ip, for each daytime, the cell\n",
    "from which most queries have been made.\n",
    "It could be useful to individuate home/work cell id in grid.\n",
    "The assumption here is that users are on average\n",
    "habitudinary people, such that each day, for each daytime,\n",
    "the set of cells does not vary too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#reset the multi index manually\n",
    "#i.e just removed first two rows from stats2 and created header\n",
    "\n",
    "!sed -i -e 1,3d max_cell_daytime.csv\n",
    "!echo -e \"ip_address,daytime,cell_id,count\\n$(cat max_cell_daytime.csv)\" > max_cell_daytime.csv\n",
    "\n",
    "max_cell_daytime = pd.read_csv('max_cell_daytime.csv', sep=',', header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm queries_filtered.csv\n",
    "class Filter:\n",
    "    def __init__(self, max_cell_daytime):\n",
    "        self.max_cell_daytime = max_cell_daytime\n",
    "\n",
    "    def filter_locations(self, user, daytime, cell_id):\n",
    "        max_cell_daytime = self.max_cell_daytime\n",
    "        cell = max_cell_daytime.loc[(max_cell_daytime['ip_address'] == user) & (max_cell_daytime['daytime'] == daytime)]['cell_id'].to_numpy()[0]\n",
    "        if cell_id != cell:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "custom_filter = Filter(max_cell_daytime)\n",
    "queries_df['filter'] = np.vectorize(custom_filter.filter_locations)(queries_df['ip_address'], queries_df['daytime'], queries_df['cell_id'])\n",
    "queries_df_filtered = queries_df[queries_df['filter'] == True].drop(columns=['filter'])\\\n",
    "    .sort_values(by=['ip_address', 'day', 'time', 'daytime'], axis=0)\n",
    "print(queries_df_filtered)\n",
    "queries_df_filtered.to_csv('queries_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "filtering queries with locations from cell_id which\n",
    "turned out to be the most interesting cells, for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import make_dataclass\n",
    "pattern = make_dataclass(\"pattern\", [(\"lat\", np.float32), (\"lon\", np.float32), ('time', int), (\"pois\",str)])\n",
    "trace = make_dataclass(\"trace\", [(\"user\", str), (\"day\", int), (\"pattern\", pattern)])\n",
    "traces = []\n",
    "users = queries_df['ip_address'].drop_duplicates().tolist()\n",
    "days = [_ for _ in range(1,21)]\n",
    "\n",
    "for user in users:\n",
    "    for day in days:\n",
    "        serie = queries_df[(queries_df['ip_address'] == user) & (queries_df['day'] == day)][['lat','lon','time','poi_type_query']]\n",
    "        for row in serie.itertuples():\n",
    "            pat = pattern(row[1], row[2], row[3], row[4])\n",
    "            tr = trace(user, day, pat)\n",
    "            traces.append(tr)\n",
    "\n",
    "traces_df = pd.DataFrame(traces)\n",
    "print(traces_df)\n",
    "traces_df.to_csv('traces.csv', sep=',', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not so useful stuff..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Infer users' co location using trajectory similarities\n",
    "\n",
    "### 3.1\n",
    "Here we try to study, for each user, on a daily basis, their\n",
    "trajectories and how they differ from day to day. For\n",
    "computing the similarity, Frechet distance is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install similaritymeasures\n",
    "!rm -r ./daily_trajectories\n",
    "!mkdir ./daily_trajectories\n",
    "\n",
    "from similaritymeasures import frechet_dist\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "users = queries_df['ip_address'].drop_duplicates().tolist()\n",
    "days = [_ for _ in range(1,21)]\n",
    "user_trajectories_per_day = []\n",
    "user_trajectories_cluster = []\n",
    "\n",
    "def compute_distance_matrix(trajectories):\n",
    "    n = len(trajectories)\n",
    "    dist_m = np.zeros((n, n))\n",
    "    for i in range(n - 1):\n",
    "        p = trajectories[i]\n",
    "        for j in range(i, n):\n",
    "            q = trajectories[j]\n",
    "            dist_m[i, j] = frechet_dist(np.radians(p), np.radians(q))\n",
    "            dist_m[j, i] = dist_m[i, j]\n",
    "    return dist_m\n",
    "\n",
    "def clustering_by_dbscan(distance_matrix, eps=100):\n",
    "    \"\"\"\n",
    "    :param eps: unit m for Frechet distance. Should be converted from radians\n",
    "    \"\"\"\n",
    "    db = DBSCAN(eps=eps, min_samples=1, metric='precomputed').fit(distance_matrix)\n",
    "    return db.labels_\n",
    "\n",
    "for user in users:\n",
    "    max_points = queries_df[(queries_df['ip_address'] == user)].groupby('day').size().agg(['max'])\n",
    "    max_points = max_points.to_numpy()[0]\n",
    "    trajectories_per_day = []\n",
    "    ticks = []\n",
    "    for day in days:\n",
    "        serie = queries_df[(queries_df['ip_address'] == user) & (queries_df['day'] == day)][['lat','lon']]\n",
    "        x = []\n",
    "        y = []\n",
    "        for row in serie.itertuples():\n",
    "            x.append(row[1])\n",
    "            y.append(row[2])\n",
    "        if len(x) != 0 and len(y) != 0:\n",
    "            ticks.append(str(day))\n",
    "            while len(x) < max_points:\n",
    "                #fill trajectory with last known position\n",
    "                x.append(x[-1])\n",
    "                y.append(y[-1])\n",
    "            assert len(x) == max_points\n",
    "            assert len(y) == max_points\n",
    "            trajectory = np.zeros((max_points,2))\n",
    "            trajectory[:,0] = x\n",
    "            trajectory[:,1] = y\n",
    "            trajectories_per_day.append(trajectory)\n",
    "    dist_m = compute_distance_matrix(trajectories_per_day)\n",
    "    sns.set(font_scale = 2)\n",
    "    fig = plt.figure(figsize=(42,21))\n",
    "    ax = sns.heatmap(dist_m, linewidths=1,\n",
    "                     linecolor='white',\n",
    "                     xticklabels=ticks,\n",
    "                     yticklabels=ticks)\n",
    "    plt.savefig(f'./daily_trajectories/{user}-matrix.png')\n",
    "    plt.close()\n",
    "    user_trajectories_per_day.append(trajectories_per_day)\n",
    "\n",
    "    labels = clustering_by_dbscan(dist_m)\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    assert n_clusters_ == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Results: User trajectories\n",
    "are almost the same on a daily basis. In order to validate the results,\n",
    "we used dbscan clustering and checked if the number of clusters\n",
    "of trajectories per user was 1.\n",
    "Frechet distance is a good metric.\n",
    "\n",
    "### 3.2\n",
    "Here we compare users trajectory to see\n",
    "if some users have commons patterns. In order to reduce the\n",
    "number of users compared, we considered one day at time.\n",
    "\n",
    "### Adversarial Model\n",
    "The adversary is the service provider.\n",
    "#### Passive adversary (Honest but curious).\n",
    "The adversary is in possess of\n",
    "the logs of all the queries made by users to the service,\n",
    "containing queries contents and metadata.\n",
    "#### Global adversary:\n",
    "the adversary view is the whole network (i.e all grids\n",
    "covered by the service).\n",
    "#### Computational power:\n",
    "the adversary is bounded by polynomial time complexity.\n",
    "#### Background knowledge:\n",
    "The adversary has some background knowledge on a subset of users, for example\n",
    "she knows the mapping between IP address and names.\n",
    "She has also access to side information on users, for example she\n",
    "can observe their social network profiles.\n",
    "#### Adversarial Goal:\n",
    "Gain knowledge about daily similarities in users trajectories, such that\n",
    "to identify users through co-locations information.\n",
    "For example, if the adversary learns that on day 5 user 1, Bob, and user 2,\n",
    "unknown, had very similar trajectories, and she furthermore knows Bob name and surname\n",
    "(thus having access to a public profile for this user on a social network), if\n",
    "Bob shares information about co-location (for example being at lunch\n",
    "with his friend Alice), then the adversary can infer that user 2 is Alice.\n",
    "(From https://ieeexplore.ieee.org/document/8228621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_user_id(user):\n",
    "    return users.index(user)\n",
    "\n",
    "queries_df['user_id'] = np.vectorize(generate_user_id)(queries_df['ip_address'])\n",
    "print(queries_df)\n",
    "users_trajectories = []\n",
    "cells = queries_df['cell_id'].drop_duplicates().tolist()\n",
    "data = []\n",
    "for day in days:\n",
    "    #find longest trajectory\n",
    "    max_points = queries_df[queries_df['day'] == day]\\\n",
    "        .groupby('ip_address').size().agg(['max'])\n",
    "    max_points = max_points.to_numpy()[0]\n",
    "    trajectories_per_user = []\n",
    "    ticks = []\n",
    "    for user in users:\n",
    "        serie = queries_df[\n",
    "            (queries_df['ip_address'] == user)\n",
    "            &\n",
    "            (queries_df['day'] == day)][['lat','lon']]\n",
    "        x = []\n",
    "        y = []\n",
    "        for row in serie.itertuples():\n",
    "            x.append(row[1])\n",
    "            y.append(row[2])\n",
    "        if len(x) > max_points/2:\n",
    "            ticks.append(str(users.index(user))) #user id\n",
    "            while len(x) < max_points:\n",
    "                #fill trajectory with last known position\n",
    "                x.append(x[-1])\n",
    "                y.append(y[-1])\n",
    "            assert len(x) == max_points\n",
    "            assert len(y) == max_points\n",
    "            trajectory = np.zeros((max_points,2))\n",
    "            trajectory[:,0] = x\n",
    "            trajectory[:,1] = y\n",
    "            trajectories_per_user.append(trajectory)\n",
    "    if len(trajectories_per_user) >= 2:\n",
    "        dist_m = compute_distance_matrix(trajectories_per_user)\n",
    "        km = 0.1 #100 meter difference between trajectories\n",
    "        km_per_radians = 6731.0\n",
    "        labels = clustering_by_dbscan(dist_m,eps=km/km_per_radians)\n",
    "        for i in range(0,len(labels)):\n",
    "            for j in range(i + 1, len(labels)-1):\n",
    "                if labels[i] == labels[j] and labels[i] != -1:\n",
    "                    print(f'{users[i]} similar to {users[j]} on day {day}')\n",
    "                    data.append( (users[i], users[j], day))\n",
    "clusterized_users = pd.DataFrame(data, columns=[\"user_A\", \"user_B\", \"day\"])\n",
    "clusterized_users.to_csv('users_trajectories_similarities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Infer Users' Top Locations\n",
    "\n",
    "What we are trying to do here is: for each user, for each daytime group,\n",
    "we want to cluster their locations. By doing so, we can identify,\n",
    "for example, their top locations. Their house would be for\n",
    "example the centroid of the cluster of locations with most points in daytime group = home.\n",
    "\n",
    "### Adversarial Model\n",
    "The adversary is the service provider.\n",
    "#### Passive adversary (Honest but curious).\n",
    "The adversary is in possess of\n",
    "the logs of all the queries made by users to the service, containing queries contents\n",
    "and metadata.\n",
    "#### Global adversary:\n",
    "the adversary view is the whole network (i.e all grids\n",
    "covered by the service).\n",
    "#### Computational power:\n",
    "the adversary is bounded by polynomial time complexity.\n",
    "#### Background knowledge:\n",
    "If the adversary has some background knowledge on users, for example\n",
    "she knows the mapping between IP address and names, an identity linkage attack\n",
    "is possible.\n",
    "#### Adversarial Goal:\n",
    "Infer users' top locations (i.e home, workplace,\n",
    "frequent places visited during leisure time) to later be able\n",
    "to re-identify users (identity inference attack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!rm -r ./location_clusters\n",
    "!mkdir ./location_clusters\n",
    "\n",
    "daytime = {}\n",
    "daytime['home'] = ['1.Early', '5.Night']\n",
    "daytime['work'] = ['2.Morning', '3.Afternoon']\n",
    "daytime['leisure'] = ['4.Evening']\n",
    "\n",
    "data = []\n",
    "for user in users:\n",
    "    entry = []\n",
    "    entry.append(user)\n",
    "    for dt in daytime.keys():\n",
    "        for moment in daytime[dt]:\n",
    "            serie = queries_df[(queries_df['ip_address'] == user) & (queries_df['daytime'] == moment)][['lat','lon']]\n",
    "        x = []\n",
    "        y = []\n",
    "        for row in serie.itertuples():\n",
    "            x.append(row[1])\n",
    "            y.append(row[2])\n",
    "        n = len(x)\n",
    "        if n > 2:\n",
    "            coords = np.zeros((n,2))\n",
    "            coords[:,0] = x\n",
    "            coords[:,1] = y\n",
    "\n",
    "            #convert to randians\n",
    "            coords = np.radians(coords)\n",
    "            km = 0.01 #10 meter accuracy\n",
    "            km_per_radians = 6731.0\n",
    "            db = DBSCAN(eps=km/km_per_radians, min_samples=2, algorithm='ball_tree', metric='haversine').fit(coords)\n",
    "            fig = plt.figure()\n",
    "            fig.set_size_inches(10,8)\n",
    "            plt.scatter(x,y,c=db.labels_)\n",
    "            plt.savefig(f\"./location_clusters/{user}_{dt}.png\")\n",
    "            plt.close()\n",
    "\n",
    "            clusters_noise = np.array(db.labels_)\n",
    "            clusters = np.array([c for c in clusters_noise if c >= 0])\n",
    "            count = np.bincount(clusters)\n",
    "            top_cluster = clusters[np.argmax(count)]\n",
    "            x_top = []\n",
    "            y_top = []\n",
    "            for i in range(0,len(clusters_noise)):\n",
    "                if clusters_noise[i] == top_cluster:\n",
    "                    x_top.append(x[i])\n",
    "                    y_top.append(y[i])\n",
    "\n",
    "            lat = np.array(x_top).mean()\n",
    "            lon = np.array(y_top).mean()\n",
    "\n",
    "            entry.append(lat)\n",
    "            entry.append(lon)\n",
    "    data.append(entry)\n",
    "top_loc = pd.DataFrame(data, columns=\n",
    "                       ['ip_address', 'home_lat','home_lon','work_lat','work_lon', 'leisure_lan', 'leisure_lon'])\n",
    "print(top_loc)\n",
    "top_loc.to_csv('top_locations.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
